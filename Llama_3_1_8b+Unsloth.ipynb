{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "6b98b76c-6982-4a56-f797-7e1d61dcaf31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "# fourbit_models = [\n",
        "#     \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "#     \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "#     \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "#     \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "#     \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "#     \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "#     \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "#     \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "#     \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "#     \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "# ] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "148a52d8-bb1d-47f0-93c6-c952384db170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8r1r5L6myo3",
        "outputId": "204c69f3-d69a-4772-977e-62c802fefb8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DkQytMiZyJjC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load your custom dataset (adjust the path as necessary)\n",
        "file_path = '/content/drive/MyDrive/llama3_8b_finetune_data/dl_train_data.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    custom_dataset = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bP6LXaXAyKNk"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the JSON data to a pandas DataFrame\n",
        "custom_dataset_df = pd.DataFrame(custom_dataset)\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face dataset\n",
        "hf_dataset = Dataset.from_pandas(custom_dataset_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "32752cdd3dfb4a91aafa8ffaae733b1d",
            "c9798ac9ba4d4c92b74c5cff8bd8608f",
            "1c729dae75754d06a15b3fdc20d8077f",
            "ab274a8d1c90445ca214d946e3eb15f6",
            "9c75d3a8bb414dbe93cc834607a4426c",
            "6326d80446494f5bab53c5e538027892",
            "32c403061caa4b9abff49482799fa0a3",
            "d9a1864eedea4dddb52e4ad10d0dfd24",
            "bdfffedb05b44427abab0dff5327d3d7",
            "c1d29a1604044b7ca5fd99103f81af19",
            "5794f691528141288f65994ddc0c88c0"
          ]
        },
        "id": "l62mhgkmyNNR",
        "outputId": "6c07156c-033c-4c77-b803-dfc6f6b2ec5a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32752cdd3dfb4a91aafa8ffaae733b1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5036 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Only return the triplet in the response.\n",
            "\n",
            "### Instruction:\n",
            "In this Gene-Gene relation extraction task,you need to extract the (geneA, relation, geneB) triplet from the text, such as: (AKT, inhibit, TSC). The second element in the triple means the realation that the geneA has with geneB. Types of relations are: inhibit, be inhibited by, activate, be activated by. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triplet, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'. You will be provided with a text consists of a sentence and a gene pair in the format of (geneA, geneB). You need to classify the relation between the gene pair from the sentence and return me a (geneA, relation, geneB) triplet. For example, the text inpput is 'The genetic tapestry unfolds as AKT inhibits the vibrant threads of expression within TSC. (AKT, TSC)', and you need to give me the output (AKT, inhibit, TSC).\n",
            "\n",
            "### Input:\n",
            "Therefore, the function of PI3K activates AKT signalling pathway by proteasome function might be used to increase sensitivity and or to overcome resistance(AKT,PI3K)\n",
            "\n",
            "### Response:\n",
            "(AKT, activate, PI3K)<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "\n",
        "# Define the EOS token\n",
        "EOS_TOKEN = tokenizer.eos_token if tokenizer.eos_token else '<|endoftext|>'\n",
        "\n",
        "# Define the formatting function\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Only return the triplet in the response.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply the formatting function to the dataset\n",
        "formatted_dataset = hf_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Verify the formatting\n",
        "print(formatted_dataset['text'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "5df49a67ddfd4bf4b4264127edf5e760",
            "06290fd190644cda83f50eba5624982b",
            "d552ea4085274fd8be06d25e6f7b77ca",
            "a30bba8bdca9427faa44e87d6cec415f",
            "2bf55233b9a74fd588328c04ca07da50",
            "62f44855111d40f493358f6e2936caad",
            "18ce2c6a5b58456dadf2845251358137",
            "2af8fbe208c240b79b6d37e0b6ef84b3",
            "d4e3ac1a97ce4cbc80360b5c2969d602",
            "6d24e4a729d6482faaf8f56162785916",
            "ffaa445c22274e4b9dc08606839093c9"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "e8e55c78-e4f6-48d2-9b5f-5b1b11383545"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5df49a67ddfd4bf4b4264127edf5e760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/5036 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "31af799f-5108-4d0c-e63d-458f14b00214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.984 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "8bd9950c-10c8-4a54-8acb-8aec3bf44d93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 5,036 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 08:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.342700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.304800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.191300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.992200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.763700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.439700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.724400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.539600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.367300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.236900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.204000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.182400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.169300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.142300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.162200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.188100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.170900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.189300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.172100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.130900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.173400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.159600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.166700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.151100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.148800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.113100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.114600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.113900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.140500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.130100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.101000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.130500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.119500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.103500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.101900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.111000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.107300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.103500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.100800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "a286ede0-f0e2-4557-c72c-99186f7750f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "441.382 seconds used for training.\n",
            "7.36 minutes used for training.\n",
            "Peak reserved memory = 7.902 GB.\n",
            "Peak reserved memory for training = 1.918 GB.\n",
            "Peak reserved memory % of max memory = 53.58 %.\n",
            "Peak reserved memory for training % of max memory = 13.005 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "bedd2386-5851-4c94-b6ba-31e820b02982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "In this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'. Please return me 'None' if there's no gene-gene relationship in the text.\n",
            "\n",
            "### Input:\n",
            "Atopic dermatitis (AD) is characterized by a defective skin barrier which allows increased allergen and pathogen penetration. Loricrin (LOR) and involucrin (IVL) are proteins important for skin barrier formation and integrity. In this study, we demonstrate that the gene and protein expression of LOR and IVL is significantly decreased in involved (LOR: p<0.001; IVL: p<0.001) and uninvolved (LOR: p<0.001; IVL: p<0.001) skin of AD subjects, as compared to skin from healthy subjects. Using primary keratinocytes, we further demonstrate the down-regulatory effect of IL-4 and IL-13--which are over-expressed in the skin of AD patients--on LOR and IVL expression in keratinocytes. Additionally, skin biopsies from signal transducer and activator of transcription (STAT)-6 transgenic mice were deficient in the expression and production of LOR and IVL. This study suggests that Th2 cytokines inhibit expression of LOR and IVL through a STAT-6 dependent mechanism.\n",
            "\n",
            "### Response:\n",
            "LOR, be inhibited by, IVL\n",
            "\n",
            "### Explanation:\n",
            "LOR and IVL are proteins important for skin barrier formation and integrity. In this study, we demonstrate that the gene and protein expression of LOR and IVL is significantly decreased in involved (LOR: p<0.001; IVL: p<0.001) and uninvolved (LOR: p<0.001; IVL: p<0.001) skin of AD subjects, as compared to skin from healthy subjects. Using primary keratinocytes, we further demonstrate the down-regulatory effect of IL-4 and IL-13\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"In this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'. Please return me 'None' if there's no gene-gene relationship in the text.\", # instruction\n",
        "        \"Atopic dermatitis (AD) is characterized by a defective skin barrier which allows increased allergen and pathogen penetration. Loricrin (LOR) and involucrin (IVL) are proteins important for skin barrier formation and integrity. In this study, we demonstrate that the gene and protein expression of LOR and IVL is significantly decreased in involved (LOR: p<0.001; IVL: p<0.001) and uninvolved (LOR: p<0.001; IVL: p<0.001) skin of AD subjects, as compared to skin from healthy subjects. Using primary keratinocytes, we further demonstrate the down-regulatory effect of IL-4 and IL-13--which are over-expressed in the skin of AD patients--on LOR and IVL expression in keratinocytes. Additionally, skin biopsies from signal transducer and activator of transcription (STAT)-6 transgenic mice were deficient in the expression and production of LOR and IVL. This study suggests that Th2 cytokines inhibit expression of LOR and IVL through a STAT-6 dependent mechanism.\", # insput\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvX0JaGmX5z",
        "outputId": "ca96421a-9ff8-4148-bb9a-129410d90b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data saved to /content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_20small_output.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# å®šä¹‰ Alpaca æ ¼å¼çš„ prompt æ¨¡æ¿\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# è¯»å– JSONL æ–‡ä»¶\n",
        "input_file = \"/content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_20small.jsonl\"\n",
        "output_file = \"/content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_20small_output.jsonl\"\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "for entry in data:\n",
        "    instruction = \"In this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'.\"\n",
        "    input_text = entry[\"Abstract\"]\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "\n",
        "    # ç”Ÿæˆæ¨¡åž‹è¾“å…¥\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # ç”Ÿæˆè¾“å‡º\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # æå–ç”Ÿæˆæ–‡æœ¬çš„å“åº”éƒ¨åˆ†\n",
        "    response_text = generated_text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # æ›´æ–° \"predict\" å­—æ®µ\n",
        "    entry[\"predict\"] = response_text\n",
        "\n",
        "# å°†å¤„ç†åŽçš„æ•°æ®å†™å›žæ–‡ä»¶\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Processed data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "upcOlWe7A1vc",
        "outputId": "c4b7aab1-3d0a-4a8b-f33d-390dbb6cfd4e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3d0f55e90e51>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/lora_model3.1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Local saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/lora_model3.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m                 \u001b[0;31m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m                 \u001b[0;31m# joyfulness), but for now this enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m                 \u001b[0msafe_save_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m                 \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \"\"\"\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mserialize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    489\u001b[0m         k: {\n\u001b[1;32m    490\u001b[0m             \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         }\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/lora_model3.1\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/lora_model3.1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "9708a7d4-b27d-44ba-a770-32d85ad9cc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.Only return the triplet in the response.\\n\\n### Instruction:\\nIn this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'. Please return me 'None' if there's no gene-gene relationship in the text.\\n\\n### Input:\\nAtopic dermatitis (AD) is characterized by a defective skin barrier which allows increased allergen and pathogen penetration. Loricrin (LOR) and involucrin (IVL) are proteins important for skin barrier formation and integrity. In this study, we demonstrate that the gene and protein expression of LOR and IVL is significantly decreased in involved (LOR: p<0.001; IVL: p<0.001) and uninvolved (LOR: p<0.001; IVL: p<0.001) skin of AD subjects, as compared to skin from healthy subjects. Using primary keratinocytes, we further demonstrate the down-regulatory effect of IL-4 and IL-13--which are over-expressed in the skin of AD patients--on LOR and IVL expression in keratinocytes. Additionally, skin biopsies from signal transducer and activator of transcription (STAT)-6 transgenic mice were deficient in the expression and production of LOR and IVL. This study suggests that Th2 cytokines inhibit expression of LOR and IVL through a STAT-6 dependent mechanism.\\n\\n### Response:\\n(LOR, be inhibited by, STAT-6),(STAT-6, inhibit, LOR),(LOR, be inhibited by, IL-4),(IL-4, inhibit, LOR),(LOR, be inhibited by, IL-13),(IL-13, inhibit, LOR),(LOR, be inhibited by, Th2 cytokines),(Th2 cytokines, inhibit, LOR),(LOR, be inhibited by, STAT-6 dependent mechanism),(STAT-6 dependent mechanism, inhibit, LOR),(LOR, be inhibited by, expression),(expression, inhibit, LOR),(LOR, be inhibited by\"]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/lora_model3.1\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.Only return the triplet in the response.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"In this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'. Please return me 'None' if there's no gene-gene relationship in the text.\", # instruction\n",
        "        \"Atopic dermatitis (AD) is characterized by a defective skin barrier which allows increased allergen and pathogen penetration. Loricrin (LOR) and involucrin (IVL) are proteins important for skin barrier formation and integrity. In this study, we demonstrate that the gene and protein expression of LOR and IVL is significantly decreased in involved (LOR: p<0.001; IVL: p<0.001) and uninvolved (LOR: p<0.001; IVL: p<0.001) skin of AD subjects, as compared to skin from healthy subjects. Using primary keratinocytes, we further demonstrate the down-regulatory effect of IL-4 and IL-13--which are over-expressed in the skin of AD patients--on LOR and IVL expression in keratinocytes. Additionally, skin biopsies from signal transducer and activator of transcription (STAT)-6 transgenic mice were deficient in the expression and production of LOR and IVL. This study suggests that Th2 cytokines inhibit expression of LOR and IVL through a STAT-6 dependent mechanism.\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dAkGV2fMVLd",
        "outputId": "30e7e171-fac6-47b8-ed7f-2d38b88b81ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing entry 1 / 500\n",
            "Processing entry 2 / 500\n",
            "Processing entry 3 / 500\n",
            "Processing entry 4 / 500\n",
            "Processing entry 5 / 500\n",
            "Processing entry 6 / 500\n",
            "Processing entry 7 / 500\n",
            "Processing entry 8 / 500\n",
            "Processing entry 9 / 500\n",
            "Processing entry 10 / 500\n",
            "Processing entry 11 / 500\n",
            "Processing entry 12 / 500\n",
            "Processing entry 13 / 500\n",
            "Processing entry 14 / 500\n",
            "Processing entry 15 / 500\n",
            "Processing entry 16 / 500\n",
            "Processing entry 17 / 500\n",
            "Processing entry 18 / 500\n",
            "Processing entry 19 / 500\n",
            "Processing entry 20 / 500\n",
            "Processing entry 21 / 500\n",
            "Processing entry 22 / 500\n",
            "Processing entry 23 / 500\n",
            "Processing entry 24 / 500\n",
            "Processing entry 25 / 500\n",
            "Processing entry 26 / 500\n",
            "Processing entry 27 / 500\n",
            "Processing entry 28 / 500\n",
            "Processing entry 29 / 500\n",
            "Processing entry 30 / 500\n",
            "Processing entry 31 / 500\n",
            "Processing entry 32 / 500\n",
            "Processing entry 33 / 500\n",
            "Processing entry 34 / 500\n",
            "Processing entry 35 / 500\n",
            "Processing entry 36 / 500\n",
            "Processing entry 37 / 500\n",
            "Processing entry 38 / 500\n",
            "Processing entry 39 / 500\n",
            "Processing entry 40 / 500\n",
            "Processing entry 41 / 500\n",
            "Processing entry 42 / 500\n",
            "Processing entry 43 / 500\n",
            "Processing entry 44 / 500\n",
            "Processing entry 45 / 500\n",
            "Processing entry 46 / 500\n",
            "Processing entry 47 / 500\n",
            "Processing entry 48 / 500\n",
            "Processing entry 49 / 500\n",
            "Processing entry 50 / 500\n",
            "Processing entry 51 / 500\n",
            "Processing entry 52 / 500\n",
            "Processing entry 53 / 500\n",
            "Processing entry 54 / 500\n",
            "Processing entry 55 / 500\n",
            "Processing entry 56 / 500\n",
            "Processing entry 57 / 500\n",
            "Processing entry 58 / 500\n",
            "Processing entry 59 / 500\n",
            "Processing entry 60 / 500\n",
            "Processing entry 61 / 500\n",
            "Processing entry 62 / 500\n",
            "Processing entry 63 / 500\n",
            "Processing entry 64 / 500\n",
            "Processing entry 65 / 500\n",
            "Processing entry 66 / 500\n",
            "Processing entry 67 / 500\n",
            "Processing entry 68 / 500\n",
            "Processing entry 69 / 500\n",
            "Processing entry 70 / 500\n",
            "Processing entry 71 / 500\n",
            "Processing entry 72 / 500\n",
            "Processing entry 73 / 500\n",
            "Processing entry 74 / 500\n",
            "Processing entry 75 / 500\n",
            "Processing entry 76 / 500\n",
            "Processing entry 77 / 500\n",
            "Processing entry 78 / 500\n",
            "Processing entry 79 / 500\n",
            "Processing entry 80 / 500\n",
            "Processing entry 81 / 500\n",
            "Processing entry 82 / 500\n",
            "Processing entry 83 / 500\n",
            "Processing entry 84 / 500\n",
            "Processing entry 85 / 500\n",
            "Processing entry 86 / 500\n",
            "Processing entry 87 / 500\n",
            "Processing entry 88 / 500\n",
            "Processing entry 89 / 500\n",
            "Processing entry 90 / 500\n",
            "Processing entry 91 / 500\n",
            "Processing entry 92 / 500\n",
            "Processing entry 93 / 500\n",
            "Processing entry 94 / 500\n",
            "Processing entry 95 / 500\n",
            "Processing entry 96 / 500\n",
            "Processing entry 97 / 500\n",
            "Processing entry 98 / 500\n",
            "Processing entry 99 / 500\n",
            "Processing entry 100 / 500\n",
            "Processing entry 101 / 500\n",
            "Processing entry 102 / 500\n",
            "Processing entry 103 / 500\n",
            "Processing entry 104 / 500\n",
            "Processing entry 105 / 500\n",
            "Processing entry 106 / 500\n",
            "Processing entry 107 / 500\n",
            "Processing entry 108 / 500\n",
            "Processing entry 109 / 500\n",
            "Processing entry 110 / 500\n",
            "Processing entry 111 / 500\n",
            "Processing entry 112 / 500\n",
            "Processing entry 113 / 500\n",
            "Processing entry 114 / 500\n",
            "Processing entry 115 / 500\n",
            "Processing entry 116 / 500\n",
            "Processing entry 117 / 500\n",
            "Processing entry 118 / 500\n",
            "Processing entry 119 / 500\n",
            "Processing entry 120 / 500\n",
            "Processing entry 121 / 500\n",
            "Processing entry 122 / 500\n",
            "Processing entry 123 / 500\n",
            "Processing entry 124 / 500\n",
            "Processing entry 125 / 500\n",
            "Processing entry 126 / 500\n",
            "Processing entry 127 / 500\n",
            "Processing entry 128 / 500\n",
            "Processing entry 129 / 500\n",
            "Processing entry 130 / 500\n",
            "Processing entry 131 / 500\n",
            "Processing entry 132 / 500\n",
            "Processing entry 133 / 500\n",
            "Processing entry 134 / 500\n",
            "Processing entry 135 / 500\n",
            "Processing entry 136 / 500\n",
            "Processing entry 137 / 500\n",
            "Processing entry 138 / 500\n",
            "Processing entry 139 / 500\n",
            "Processing entry 140 / 500\n",
            "Processing entry 141 / 500\n",
            "Processing entry 142 / 500\n",
            "Processing entry 143 / 500\n",
            "Processing entry 144 / 500\n",
            "Processing entry 145 / 500\n",
            "Processing entry 146 / 500\n",
            "Processing entry 147 / 500\n",
            "Processing entry 148 / 500\n",
            "Processing entry 149 / 500\n",
            "Processing entry 150 / 500\n",
            "Processing entry 151 / 500\n",
            "Processing entry 152 / 500\n",
            "Processing entry 153 / 500\n",
            "Processing entry 154 / 500\n",
            "Processing entry 155 / 500\n",
            "Processing entry 156 / 500\n",
            "Processing entry 157 / 500\n",
            "Processing entry 158 / 500\n",
            "Processing entry 159 / 500\n",
            "Processing entry 160 / 500\n",
            "Processing entry 161 / 500\n",
            "Processing entry 162 / 500\n",
            "Processing entry 163 / 500\n",
            "Processing entry 164 / 500\n",
            "Processing entry 165 / 500\n",
            "Processing entry 166 / 500\n",
            "Processing entry 167 / 500\n",
            "Processing entry 168 / 500\n",
            "Processing entry 169 / 500\n",
            "Processing entry 170 / 500\n",
            "Processing entry 171 / 500\n",
            "Processing entry 172 / 500\n",
            "Processing entry 173 / 500\n",
            "Processing entry 174 / 500\n",
            "Processing entry 175 / 500\n",
            "Processing entry 176 / 500\n",
            "Processing entry 177 / 500\n",
            "Processing entry 178 / 500\n",
            "Processing entry 179 / 500\n",
            "Processing entry 180 / 500\n",
            "Processing entry 181 / 500\n",
            "Processing entry 182 / 500\n",
            "Processing entry 183 / 500\n",
            "Processing entry 184 / 500\n",
            "Processing entry 185 / 500\n",
            "Processing entry 186 / 500\n",
            "Processing entry 187 / 500\n",
            "Processing entry 188 / 500\n",
            "Processing entry 189 / 500\n",
            "Processing entry 190 / 500\n",
            "Processing entry 191 / 500\n",
            "Processing entry 192 / 500\n",
            "Processing entry 193 / 500\n",
            "Processing entry 194 / 500\n",
            "Processing entry 195 / 500\n",
            "Processing entry 196 / 500\n",
            "Processing entry 197 / 500\n",
            "Processing entry 198 / 500\n",
            "Processing entry 199 / 500\n",
            "Processing entry 200 / 500\n",
            "Processing entry 201 / 500\n",
            "Processing entry 202 / 500\n",
            "Processing entry 203 / 500\n",
            "Processing entry 204 / 500\n",
            "Processing entry 205 / 500\n",
            "Processing entry 206 / 500\n",
            "Processing entry 207 / 500\n",
            "Processing entry 208 / 500\n",
            "Processing entry 209 / 500\n",
            "Processing entry 210 / 500\n",
            "Processing entry 211 / 500\n",
            "Processing entry 212 / 500\n",
            "Processing entry 213 / 500\n",
            "Processing entry 214 / 500\n",
            "Processing entry 215 / 500\n",
            "Processing entry 216 / 500\n",
            "Processing entry 217 / 500\n",
            "Processing entry 218 / 500\n",
            "Processing entry 219 / 500\n",
            "Processing entry 220 / 500\n",
            "Processing entry 221 / 500\n",
            "Processing entry 222 / 500\n",
            "Processing entry 223 / 500\n",
            "Processing entry 224 / 500\n",
            "Processing entry 225 / 500\n",
            "Processing entry 226 / 500\n",
            "Processing entry 227 / 500\n",
            "Processing entry 228 / 500\n",
            "Processing entry 229 / 500\n",
            "Processing entry 230 / 500\n",
            "Processing entry 231 / 500\n",
            "Processing entry 232 / 500\n",
            "Processing entry 233 / 500\n",
            "Processing entry 234 / 500\n",
            "Processing entry 235 / 500\n",
            "Processing entry 236 / 500\n",
            "Processing entry 237 / 500\n",
            "Processing entry 238 / 500\n",
            "Processing entry 239 / 500\n",
            "Processing entry 240 / 500\n",
            "Processing entry 241 / 500\n",
            "Processing entry 242 / 500\n",
            "Processing entry 243 / 500\n",
            "Processing entry 244 / 500\n",
            "Processing entry 245 / 500\n",
            "Processing entry 246 / 500\n",
            "Processing entry 247 / 500\n",
            "Processing entry 248 / 500\n",
            "Processing entry 249 / 500\n",
            "Processing entry 250 / 500\n",
            "Processing entry 251 / 500\n",
            "Processing entry 252 / 500\n",
            "Processing entry 253 / 500\n",
            "Processing entry 254 / 500\n",
            "Processing entry 255 / 500\n",
            "Processing entry 256 / 500\n",
            "Processing entry 257 / 500\n",
            "Processing entry 258 / 500\n",
            "Processing entry 259 / 500\n",
            "Processing entry 260 / 500\n",
            "Processing entry 261 / 500\n",
            "Processing entry 262 / 500\n",
            "Processing entry 263 / 500\n",
            "Processing entry 264 / 500\n",
            "Processing entry 265 / 500\n",
            "Processing entry 266 / 500\n",
            "Processing entry 267 / 500\n",
            "Processing entry 268 / 500\n",
            "Processing entry 269 / 500\n",
            "Processing entry 270 / 500\n",
            "Processing entry 271 / 500\n",
            "Processing entry 272 / 500\n",
            "Processing entry 273 / 500\n",
            "Processing entry 274 / 500\n",
            "Processing entry 275 / 500\n",
            "Processing entry 276 / 500\n",
            "Processing entry 277 / 500\n",
            "Processing entry 278 / 500\n",
            "Processing entry 279 / 500\n",
            "Processing entry 280 / 500\n",
            "Processing entry 281 / 500\n",
            "Processing entry 282 / 500\n",
            "Processing entry 283 / 500\n",
            "Processing entry 284 / 500\n",
            "Processing entry 285 / 500\n",
            "Processing entry 286 / 500\n",
            "Processing entry 287 / 500\n",
            "Processing entry 288 / 500\n",
            "Processing entry 289 / 500\n",
            "Processing entry 290 / 500\n",
            "Processing entry 291 / 500\n",
            "Processing entry 292 / 500\n",
            "Processing entry 293 / 500\n",
            "Processing entry 294 / 500\n",
            "Processing entry 295 / 500\n",
            "Processing entry 296 / 500\n",
            "Processing entry 297 / 500\n",
            "Processing entry 298 / 500\n",
            "Processing entry 299 / 500\n",
            "Processing entry 300 / 500\n",
            "Processing entry 301 / 500\n",
            "Processing entry 302 / 500\n",
            "Processing entry 303 / 500\n",
            "Processing entry 304 / 500\n",
            "Processing entry 305 / 500\n",
            "Processing entry 306 / 500\n",
            "Processing entry 307 / 500\n",
            "Processing entry 308 / 500\n",
            "Processing entry 309 / 500\n",
            "Processing entry 310 / 500\n",
            "Processing entry 311 / 500\n",
            "Processing entry 312 / 500\n",
            "Processing entry 313 / 500\n",
            "Processing entry 314 / 500\n",
            "Processing entry 315 / 500\n",
            "Processing entry 316 / 500\n",
            "Processing entry 317 / 500\n",
            "Processing entry 318 / 500\n",
            "Processing entry 319 / 500\n",
            "Processing entry 320 / 500\n",
            "Processing entry 321 / 500\n",
            "Processing entry 322 / 500\n",
            "Processing entry 323 / 500\n",
            "Processing entry 324 / 500\n",
            "Processing entry 325 / 500\n",
            "Processing entry 326 / 500\n",
            "Processing entry 327 / 500\n",
            "Processing entry 328 / 500\n",
            "Processing entry 329 / 500\n",
            "Processing entry 330 / 500\n",
            "Processing entry 331 / 500\n",
            "Processing entry 332 / 500\n",
            "Processing entry 333 / 500\n",
            "Processing entry 334 / 500\n",
            "Processing entry 335 / 500\n",
            "Processing entry 336 / 500\n",
            "Processing entry 337 / 500\n",
            "Processing entry 338 / 500\n",
            "Processing entry 339 / 500\n",
            "Processing entry 340 / 500\n",
            "Processing entry 341 / 500\n",
            "Processing entry 342 / 500\n",
            "Processing entry 343 / 500\n",
            "Processing entry 344 / 500\n",
            "Processing entry 345 / 500\n",
            "Processing entry 346 / 500\n",
            "Processing entry 347 / 500\n",
            "Processing entry 348 / 500\n",
            "Processing entry 349 / 500\n",
            "Processing entry 350 / 500\n",
            "Processing entry 351 / 500\n",
            "Processing entry 352 / 500\n",
            "Processing entry 353 / 500\n",
            "Processing entry 354 / 500\n",
            "Processing entry 355 / 500\n",
            "Processing entry 356 / 500\n",
            "Processing entry 357 / 500\n",
            "Processing entry 358 / 500\n",
            "Processing entry 359 / 500\n",
            "Processing entry 360 / 500\n",
            "Processing entry 361 / 500\n",
            "Processing entry 362 / 500\n",
            "Processing entry 363 / 500\n",
            "Processing entry 364 / 500\n",
            "Processing entry 365 / 500\n",
            "Processing entry 366 / 500\n",
            "Processing entry 367 / 500\n",
            "Processing entry 368 / 500\n",
            "Processing entry 369 / 500\n",
            "Processing entry 370 / 500\n",
            "Processing entry 371 / 500\n",
            "Processing entry 372 / 500\n",
            "Processing entry 373 / 500\n",
            "Processing entry 374 / 500\n",
            "Processing entry 375 / 500\n",
            "Processing entry 376 / 500\n",
            "Processing entry 377 / 500\n",
            "Processing entry 378 / 500\n",
            "Processing entry 379 / 500\n",
            "Processing entry 380 / 500\n",
            "Processing entry 381 / 500\n",
            "Processing entry 382 / 500\n",
            "Processing entry 383 / 500\n",
            "Processing entry 384 / 500\n",
            "Processing entry 385 / 500\n",
            "Processing entry 386 / 500\n",
            "Processing entry 387 / 500\n",
            "Processing entry 388 / 500\n",
            "Processing entry 389 / 500\n",
            "Processing entry 390 / 500\n",
            "Processing entry 391 / 500\n",
            "Processing entry 392 / 500\n",
            "Processing entry 393 / 500\n",
            "Processing entry 394 / 500\n",
            "Processing entry 395 / 500\n",
            "Processing entry 396 / 500\n",
            "Processing entry 397 / 500\n",
            "Processing entry 398 / 500\n",
            "Processing entry 399 / 500\n",
            "Processing entry 400 / 500\n",
            "Processing entry 401 / 500\n",
            "Processing entry 402 / 500\n",
            "Processing entry 403 / 500\n",
            "Processing entry 404 / 500\n",
            "Processing entry 405 / 500\n",
            "Processing entry 406 / 500\n",
            "Processing entry 407 / 500\n",
            "Processing entry 408 / 500\n",
            "Processing entry 409 / 500\n",
            "Processing entry 410 / 500\n",
            "Processing entry 411 / 500\n",
            "Processing entry 412 / 500\n",
            "Processing entry 413 / 500\n",
            "Processing entry 414 / 500\n",
            "Processing entry 415 / 500\n",
            "Processing entry 416 / 500\n",
            "Processing entry 417 / 500\n",
            "Processing entry 418 / 500\n",
            "Processing entry 419 / 500\n",
            "Processing entry 420 / 500\n",
            "Processing entry 421 / 500\n",
            "Processing entry 422 / 500\n",
            "Processing entry 423 / 500\n",
            "Processing entry 424 / 500\n",
            "Processing entry 425 / 500\n",
            "Processing entry 426 / 500\n",
            "Processing entry 427 / 500\n",
            "Processing entry 428 / 500\n",
            "Processing entry 429 / 500\n",
            "Processing entry 430 / 500\n",
            "Processing entry 431 / 500\n",
            "Processing entry 432 / 500\n",
            "Processing entry 433 / 500\n",
            "Processing entry 434 / 500\n",
            "Processing entry 435 / 500\n",
            "Processing entry 436 / 500\n",
            "Processing entry 437 / 500\n",
            "Processing entry 438 / 500\n",
            "Processing entry 439 / 500\n",
            "Processing entry 440 / 500\n",
            "Processing entry 441 / 500\n",
            "Processing entry 442 / 500\n",
            "Processing entry 443 / 500\n",
            "Processing entry 444 / 500\n",
            "Processing entry 445 / 500\n",
            "Processing entry 446 / 500\n",
            "Processing entry 447 / 500\n",
            "Processing entry 448 / 500\n",
            "Processing entry 449 / 500\n",
            "Processing entry 450 / 500\n",
            "Processing entry 451 / 500\n",
            "Processing entry 452 / 500\n",
            "Processing entry 453 / 500\n",
            "Processing entry 454 / 500\n",
            "Processing entry 455 / 500\n",
            "Processing entry 456 / 500\n",
            "Processing entry 457 / 500\n",
            "Processing entry 458 / 500\n",
            "Processing entry 459 / 500\n",
            "Processing entry 460 / 500\n",
            "Processing entry 461 / 500\n",
            "Processing entry 462 / 500\n",
            "Processing entry 463 / 500\n",
            "Processing entry 464 / 500\n",
            "Processing entry 465 / 500\n",
            "Processing entry 466 / 500\n",
            "Processing entry 467 / 500\n",
            "Processing entry 468 / 500\n",
            "Processing entry 469 / 500\n",
            "Processing entry 470 / 500\n",
            "Processing entry 471 / 500\n",
            "Processing entry 472 / 500\n",
            "Processing entry 473 / 500\n",
            "Processing entry 474 / 500\n",
            "Processing entry 475 / 500\n",
            "Processing entry 476 / 500\n",
            "Processing entry 477 / 500\n",
            "Processing entry 478 / 500\n",
            "Processing entry 479 / 500\n",
            "Processing entry 480 / 500\n",
            "Processing entry 481 / 500\n",
            "Processing entry 482 / 500\n",
            "Processing entry 483 / 500\n",
            "Processing entry 484 / 500\n",
            "Processing entry 485 / 500\n",
            "Processing entry 486 / 500\n",
            "Processing entry 487 / 500\n",
            "Processing entry 488 / 500\n",
            "Processing entry 489 / 500\n",
            "Processing entry 490 / 500\n",
            "Processing entry 491 / 500\n",
            "Processing entry 492 / 500\n",
            "Processing entry 493 / 500\n",
            "Processing entry 494 / 500\n",
            "Processing entry 495 / 500\n",
            "Processing entry 496 / 500\n",
            "Processing entry 497 / 500\n",
            "Processing entry 498 / 500\n",
            "Processing entry 499 / 500\n",
            "Processing entry 500 / 500\n",
            "Processed data saved to /content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_0-500output3.1.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.Only return me triplet as the answer.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# è¯»å– JSONL æ–‡ä»¶\n",
        "input_file = \"/content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_abstract.jsonl\"\n",
        "output_file = \"/content/drive/MyDrive/llama3_8b_finetune_data/IL13_1970-2010_0-500output3.1.jsonl\"\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# åªå¤„ç†å‰500æ¡æ•°æ®\n",
        "data = data[:500]\n",
        "\n",
        "for idx, entry in enumerate(data):\n",
        "    print(f\"Processing entry {idx + 1} / {len(data)}\")\n",
        "    instruction = \"In this Gene-Gene relation extraction task, you need to follow 3 steps. You need to extract the (gene, relation, gene) triplet from the text. The second element in the triple means the relation between the two genes. Types of relations are 'activate', 'be activated by', 'inhibit', and 'be inhibited by'. Please return all the relations extracted from the text in ternary format (GENE, RELATION, GENE). If there are more than one triple, please write in this form: '(GENE, RELATION, GENE),(GENE, RELATION, GENE),......'.\"\n",
        "    input_text = entry[\"Abstract\"]\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "\n",
        "    # ç”Ÿæˆæ¨¡åž‹è¾“å…¥\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # ç”Ÿæˆè¾“å‡º\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "      generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # æå–ç”Ÿæˆæ–‡æœ¬çš„å“åº”éƒ¨åˆ†\n",
        "    response_text = generated_text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # æ›´æ–° \"predict\" å­—æ®µ\n",
        "    entry[\"predict\"] = response_text\n",
        "\n",
        "# å°†å¤„ç†åŽçš„æ•°æ®å†™å›žæ–‡ä»¶\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Processed data saved to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06290fd190644cda83f50eba5624982b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62f44855111d40f493358f6e2936caad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18ce2c6a5b58456dadf2845251358137",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "18ce2c6a5b58456dadf2845251358137": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c729dae75754d06a15b3fdc20d8077f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9a1864eedea4dddb52e4ad10d0dfd24",
            "max": 5036,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdfffedb05b44427abab0dff5327d3d7",
            "value": 5036
          }
        },
        "2af8fbe208c240b79b6d37e0b6ef84b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf55233b9a74fd588328c04ca07da50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32752cdd3dfb4a91aafa8ffaae733b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9798ac9ba4d4c92b74c5cff8bd8608f",
              "IPY_MODEL_1c729dae75754d06a15b3fdc20d8077f",
              "IPY_MODEL_ab274a8d1c90445ca214d946e3eb15f6"
            ],
            "layout": "IPY_MODEL_9c75d3a8bb414dbe93cc834607a4426c"
          }
        },
        "32c403061caa4b9abff49482799fa0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5794f691528141288f65994ddc0c88c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5df49a67ddfd4bf4b4264127edf5e760": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06290fd190644cda83f50eba5624982b",
              "IPY_MODEL_d552ea4085274fd8be06d25e6f7b77ca",
              "IPY_MODEL_a30bba8bdca9427faa44e87d6cec415f"
            ],
            "layout": "IPY_MODEL_2bf55233b9a74fd588328c04ca07da50"
          }
        },
        "62f44855111d40f493358f6e2936caad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6326d80446494f5bab53c5e538027892": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d24e4a729d6482faaf8f56162785916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c75d3a8bb414dbe93cc834607a4426c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a30bba8bdca9427faa44e87d6cec415f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d24e4a729d6482faaf8f56162785916",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ffaa445c22274e4b9dc08606839093c9",
            "value": "â€‡5036/5036â€‡[00:08&lt;00:00,â€‡808.83â€‡examples/s]"
          }
        },
        "ab274a8d1c90445ca214d946e3eb15f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d29a1604044b7ca5fd99103f81af19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5794f691528141288f65994ddc0c88c0",
            "value": "â€‡5036/5036â€‡[00:00&lt;00:00,â€‡37679.01â€‡examples/s]"
          }
        },
        "bdfffedb05b44427abab0dff5327d3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1d29a1604044b7ca5fd99103f81af19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9798ac9ba4d4c92b74c5cff8bd8608f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6326d80446494f5bab53c5e538027892",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32c403061caa4b9abff49482799fa0a3",
            "value": "Map:â€‡100%"
          }
        },
        "d4e3ac1a97ce4cbc80360b5c2969d602": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d552ea4085274fd8be06d25e6f7b77ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2af8fbe208c240b79b6d37e0b6ef84b3",
            "max": 5036,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4e3ac1a97ce4cbc80360b5c2969d602",
            "value": 5036
          }
        },
        "d9a1864eedea4dddb52e4ad10d0dfd24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffaa445c22274e4b9dc08606839093c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
