{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置Entrez的email\n",
    "Entrez.email = ''\n",
    "# Entrez.api_key=''\n",
    "# Entrez.max_tries=5\n",
    "# Entrez.sleep_between_tries=180\n",
    "# Entrez.timeout=10\n",
    "\n",
    "def search(query, start_year=None, end_year=None):\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='100000',\n",
    "                            retmode='xml',\n",
    "                            term=query,\n",
    "                            datetype='pdat' if start_year and end_year else 'none',\n",
    "                            mindate=start_year if start_year else '',\n",
    "                            maxdate=end_year if end_year else '')\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list, max_retries=5, retry_delay=5):\n",
    "    ids = ','.join(id_list)\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db='pubmed',\n",
    "                                   retmode='xml',\n",
    "                                   id=ids)\n",
    "            results = Entrez.read(handle)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching details, attempt {attempt + 1}/{max_retries}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(retry_delay)\n",
    "    raise Exception(\"Failed to fetch details after multiple attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索研究\n",
    "query = \"\" #Inflammatory Bowel Disease or Ulcerative colitis or Crohn's disease\n",
    "studies = search(query)\n",
    "total_records = int(studies['Count'])\n",
    "print(total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义时间段\n",
    "time_periods = [(1900, 1949), (1950, 1990), (1991, 2000), (2001, 2010),\n",
    "                (2011, 2015), (2016, 2020), (2021, 2025)]\n",
    "\n",
    "# 细分时间段的函数\n",
    "def split_period(start_year, end_year):\n",
    "    periods = []\n",
    "    step = 3  # 每个子时间段的步长\n",
    "    for year in range(start_year, end_year + 1, step):\n",
    "        next_year = min(year + step - 1, end_year)\n",
    "        periods.append((year, next_year))\n",
    "    return periods\n",
    "\n",
    "# 更细分时间段的函数\n",
    "def small_split_period(start_year, end_year):\n",
    "    periods = []\n",
    "    step = 1  # 每个子时间段的步长\n",
    "    for year in range(start_year, end_year + 1, step):\n",
    "        next_year = min(year + step - 1, end_year)\n",
    "        periods.append((year, next_year))\n",
    "    return periods\n",
    "\n",
    "# 初始化数据列表\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "journal_list = []\n",
    "language_list = []\n",
    "pubdate_year_list = []\n",
    "pubdate_month_list = []\n",
    "doi_list = []\n",
    "\n",
    "def process_chunk(id_list):\n",
    "    chunk_size = 1000  # 每批次请求的文章数量\n",
    "    for chunk_i in range(0, len(id_list), chunk_size):\n",
    "        chunk = id_list[chunk_i:chunk_i + chunk_size]\n",
    "        try:\n",
    "            papers = fetch_details(chunk)\n",
    "            for paper in papers['PubmedArticle']:\n",
    "                paper_id = paper['MedlineCitation']['PMID']\n",
    "                title_list.append(paper['MedlineCitation']['Article']['ArticleTitle'])\n",
    "                try:\n",
    "                    abstract_list.append(paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0])\n",
    "                except KeyError:\n",
    "                    abstract_list.append('No Abstract')\n",
    "                journal_list.append(paper['MedlineCitation']['Article']['Journal']['Title'])\n",
    "                language_list.append(paper['MedlineCitation']['Article']['Language'][0])\n",
    "                try:\n",
    "                    pubdate_year_list.append(paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year'])\n",
    "                except KeyError:\n",
    "                    pubdate_year_list.append('No Data')\n",
    "                try:\n",
    "                    pubdate_month_list.append(paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Month'])\n",
    "                except KeyError:\n",
    "                    pubdate_month_list.append('No Data')\n",
    "                try:\n",
    "                    doi = next((elocation_id for elocation_id in paper['MedlineCitation']['Article']['ELocationID'] \n",
    "                               if elocation_id.attributes['EIdType'] == 'doi'), 'No DOI')\n",
    "                except KeyError:\n",
    "                    doi = 'No DOI'\n",
    "                doi_list.append(doi)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching details for chunk {chunk_i}: {e}\")\n",
    "\n",
    "def fetch_data_for_period(start_year, end_year):\n",
    "    studies = search(query=query, start_year=start_year, end_year=end_year)\n",
    "    total_records = int(studies['Count'])\n",
    "    print(f\"# of hits for {start_year}-{end_year}: {total_records}\")\n",
    "\n",
    "    if total_records <= 10000:\n",
    "        id_list = studies[\"IdList\"]\n",
    "        process_chunk(id_list)\n",
    "    else:\n",
    "        sub_periods = split_period(start_year, end_year)\n",
    "        for sub_start_year, sub_end_year in sub_periods:\n",
    "            studies = search(query=query, start_year=sub_start_year, end_year=sub_end_year)\n",
    "            sub_total_records = int(studies['Count'])\n",
    "            print(f\"# of hits for {sub_start_year}-{sub_end_year}: {sub_total_records}\")\n",
    "            if sub_total_records == 0:\n",
    "                continue\n",
    "\n",
    "            id_list = studies[\"IdList\"]\n",
    "            if sub_total_records <= 10000:\n",
    "                process_chunk(id_list)\n",
    "            else:\n",
    "                s_sub_periods=small_split_period(sub_start_year, sub_end_year)\n",
    "                for s_sub_start_year, s_sub_end_year in s_sub_periods:\n",
    "                    studies = search(query=query, start_year=s_sub_start_year, end_year=s_sub_end_year)\n",
    "                    s_sub_total_records = int(studies['Count'])\n",
    "                    print(f\"# of hits for {s_sub_start_year}-{s_sub_end_year}: {s_sub_total_records}\")\n",
    "                    id_list = studies[\"IdList\"]\n",
    "\n",
    "                    if sub_total_records <= 10000:\n",
    "                        process_chunk(id_list)\n",
    "\n",
    "if total_records <= 10000:\n",
    "    print(\"Fetching all records...\")\n",
    "    id_list = studies[\"IdList\"]\n",
    "    process_chunk(id_list)\n",
    "else:\n",
    "    print(\"Too many hits. Splitting search by time periods...\")\n",
    "    for start_year, end_year in time_periods:\n",
    "        fetch_data_for_period(start_year, end_year)\n",
    "\n",
    "# 创建并保存DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': title_list,\n",
    "    'Abstract': abstract_list,\n",
    "    'Journal': journal_list,\n",
    "    'Language': language_list,\n",
    "    'Year': pubdate_year_list,\n",
    "    'Month': pubdate_month_list,\n",
    "    'DOI': doi_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'].replace('Jan', '01', inplace=True)\n",
    "df['Month'].replace('Feb', '02', inplace=True)\n",
    "df['Month'].replace('Mar', '03', inplace=True)\n",
    "df['Month'].replace('Apr', '04', inplace=True)\n",
    "df['Month'].replace('May', '05', inplace=True)\n",
    "df['Month'].replace('Jun', '06', inplace=True)\n",
    "df['Month'].replace('Jul', '07', inplace=True)\n",
    "df['Month'].replace('Aug', '08', inplace=True)\n",
    "df['Month'].replace('Sep', '09', inplace=True)\n",
    "df['Month'].replace('Oct', '10', inplace=True)\n",
    "df['Month'].replace('Nov', '11', inplace=True)\n",
    "df['Month'].replace('Dec', '12', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出为CSV文件\n",
    "csv_file_path = ''\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"DataFrame has been exported to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
